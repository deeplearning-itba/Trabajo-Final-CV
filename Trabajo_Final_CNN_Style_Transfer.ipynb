{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCY6UbkkI9_N"
   },
   "source": [
    "# Style Transfer\n",
    "\n",
    "<img src=\"https://i0.wp.com/chelseatroy.com/wp-content/uploads/2018/12/neural_style_transfer.png?resize=768%2C311&ssl=1\">\n",
    "\n",
    "La idea de este trabajo final es reproducir el siguiente paper:\n",
    "\n",
    "https://arxiv.org/pdf/1508.06576.pdf\n",
    "\n",
    "El objetivo es transferir el estilo de una imagen dada a otra imagen distinta. \n",
    "\n",
    "Como hemos visto en clase, las primeras capas de una red convolucional se activan ante la presencia de ciertos patrones vinculados a detalles muy pequeños.\n",
    "\n",
    "A medida que avanzamos en las distintas capas de una red neuronal convolucional, los filtros se van activando a medida que detectan patrones de formas cada vez mas complejos.\n",
    "\n",
    "Lo que propone este paper es asignarle a la activación de las primeras capas de una red neuronal convolucional (por ejemplo VGG19) la definición del estilo y a la activación de las últimas capas de la red neuronal convolucional, la definición del contenido.\n",
    "\n",
    "La idea de este paper es, a partir de dos imágenes (una que aporte el estilo y otra que aporte el contenido) analizar cómo es la activación de las primeras capas para la imagen que aporta el estilo y cómo es la activación de las últimas capas de la red convolucional para la imagen que aporta el contenido. A partir de esto se intentará sintetizar una imagen que active los filtros de las primeras capas que se activaron con la imagen que aporta el estilo y los filtros de las últimas capas que se activaron con la imagen que aporta el contenido.\n",
    "\n",
    "A este procedimiento se lo denomina neural style transfer.\n",
    "\n",
    "# En este trabajo se deberá leer el paper mencionado y en base a ello, entender la implementación que se muestra a continuación y contestar preguntas sobre la misma.\n",
    "\n",
    "# Una metodología posible es hacer una lectura rápida del paper (aunque esto signifique no entender algunos detalles del mismo) y luego ir analizando el código y respondiendo las preguntas. A medida que se planteen las preguntas, volviendo a leer secciones específicas del paper terminará de entender los detalles que pudieran haber quedado pendientes.\n",
    "\n",
    "Lo primero que haremos es cargar dos imágenes, una que aporte el estilo y otra que aporte el contenido. A tal fin utilizaremos imágenes disponibles en la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "kyHsa2t0SxZi",
    "outputId": "e72fcf52-62ed-42f1-f64e-cdb05d049797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "mkdir: /content: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Imagen para estilo\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/5/52/La_noche_estrellada1.jpg\n",
    "\n",
    "# Imagen para contenido\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Neckarfront_T%C3%BCbingen_Mai_2017.jpg/775px-Neckarfront_T%C3%BCbingen_Mai_2017.jpg\n",
    "\n",
    "# Creamos el directorio para los archivos de salida\n",
    "!mkdir /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "id": "NIxH20o2eFoc",
    "outputId": "4785bcbb-4070-4e68-c2b5-4a1dfdccbad2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iLkV1bnFl_tK"
   },
   "outputs": [],
   "source": [
    "# Definimos las imagenes que vamos a utilizar, y el directorio de salida\n",
    "\n",
    "base_image_path = Path(\"./prueba_propia/contenido.jpg\")\n",
    "style_reference_image_path = Path(\"./prueba_propia/estilo.jpg\")\n",
    "result_prefix = Path(\"./prueba_propia/output\")\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz2PeGfpeYzj"
   },
   "source": [
    "# 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "En el algoritmo del paper se propone que la loss este compuesta por dos partes. Una de ellas siendo la loss correspondiente al contenido de la una de las imagenes y la otra al estilo de la otra imagen. Tambien dentro de esta loss general se agregan factores mediante los cuales se puede pesar cuanta importantancia (peso) se le da a cada una de dichas losses (contenido y estilo).\n",
    "Estos valores \"style_weight\" y \"content_weight\", refieren al peso que tendrá cada parte de la loss, en este caso, la loss del estilo tendrá mayor peso que la del contenido.\n",
    "\n",
    "Por otro lado, \"total_variation_weight\" pesa la componente de la loss que se usa para medir el suavizado o cambios muy bruscos en la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "P9Dt3aaEmJWS"
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 0.1\n",
    "style_weight = 50\n",
    "content_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CQQJOhCVuse6"
   },
   "outputs": [],
   "source": [
    "# Definimos el tamaño de las imágenes a utilizar\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg2ct-8agm1E"
   },
   "source": [
    "# 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?\n",
    "\n",
    "Ayuda: https://keras.io/applications/\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "*- concepto general:*\n",
    "\n",
    "El proposito general es preprocesar la imagen para que este en el formato correcto para que pueda procesarse por al red VGG19\n",
    "(por ejemplo cargar la imagen, convertirla en un numpy array, pasarla a BGR y centrar cada canal de color en 0).\n",
    "\n",
    "*- load_img:*\n",
    "\n",
    "Carga la imagen que del \"image_path\", con el size dato por \"img_nrows\" e \"img_ncols\" y devuelve una instancia de la imagen en formato Python Imaging Library\n",
    "\n",
    "*- img_to_array:*\n",
    "\n",
    "Toma una imagen en formato PIL (python imaging library) y la convierte en un numpy array de 3 dimensiones (alto, ancho y RGB)\n",
    "\n",
    "*- np.expand_dims:*\n",
    "\n",
    "Agrega un nuevo axis en la posicion indicada, en este caso en axis = 0 (osea, fila)\n",
    "\n",
    "*- vgg19.preprocess_input:*\n",
    "\n",
    "Preprocesa el numpy array de la imagen obtenida y la convierte de RGB a BGR, luego cada uno de los canales respectivos a cada color es centrado en 0 para que sean acordes a los datasets y forma de entremaniento de VGG19 con ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tAkljg4zuzYd"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTf0YDSagt10"
   },
   "source": [
    "# 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Esta funcion quita el axis que se agrego para el procesamiento, dejando un formato normal de imagen,\n",
    "tambien descentra los canales de media 0 a una media dada por cada canal de color y luego modifica la imagend e BGR a RGB.\n",
    "\n",
    "Hace esto para volver del formato que necesita VGG19 al formato standard de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "y5LaTrsAu14z"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HYNio09mu4S3"
   },
   "outputs": [],
   "source": [
    "# get tensor representations of our images\n",
    "# K.variable convierte un numpy array en un tensor, para \n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "a1Lbw02Uu--o",
    "outputId": "6cc926fa-55af-43fa-fe91-3b68c0910502"
   },
   "outputs": [],
   "source": [
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJEi0YI3Uzrm"
   },
   "source": [
    "Aclaración:\n",
    "\n",
    "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gGO_jGFfvEbF"
   },
   "outputs": [],
   "source": [
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tdG59VRavHGB",
    "outputId": "a133befb-68d1-4c51-99e6-417c1103f726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG19 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70-vs_jZkKVc"
   },
   "source": [
    "# 4) En la siguientes celdas:\n",
    "\n",
    "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
    "\n",
    "La matriz gram es la herramienta que se utiliza para medir la correlacion entre features, de esta forma puede diferenciarse el estilo del contenido.\n",
    "Es el producto interno entre los vectores de feature de una capa dada.\n",
    "\n",
    "Para calcular la matriz gram se debe tomar la representacion de una capa dada L y convertila en una matriz de dos dimensiones.\n",
    "Cada fila de dicha matriz esta compuesta de toda la salida de los filtros de la capa anterior. Siendo que cada filtro busca un feature en específico, podria decirse que cada fila de la matriz representa un feature dado en distintas posiciones de la imagen\n",
    "\n",
    "Para generar una textura que coincida con la de una imagen dada, se debe usar descenso por gradiente sobre la imagen de salida (imagen compuesta por ruido blaco unicamente), para encontrar otra imagen que corresponda con la representacion del estilo de la imagen origina (osea, los features de la imagen del estilo).\n",
    "Esto se hace minimizando el MSE entre las matrices de Gram de la imagen original y la imagen a ser generada.\n",
    "\n",
    "Para generar la imagen que combine el contenido de una imagen con el estilo de la otra, se intentan minizar en conjunto los MSE de las matrices de ambos componentes (MSE sobre matrices de gram para estilo y MSE regular para contenido), pesando cada uno de estos (contenido y estilo) para darles la preponderancia que se prefiera.\n",
    "\n",
    "- ¿Por qué se permutan las dimensiones de x?\n",
    "\n",
    "Se permutan las dimensiones para que que en la matriz de gram quede el producto punto por cada uno de los canales RGB. (y finalmente la matriz de gram quede de 3 x 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "K1FODPATvJ1k"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBQkKFY0Rbx-"
   },
   "source": [
    "# 5) Losses:\n",
    "\n",
    "Explicar qué mide cada una de las losses en las siguientes tres celdas.\n",
    "\n",
    "Rta:\n",
    "\n",
    "*-style_loss:*\n",
    "\n",
    "Mide la loss del estilo, esto se calcula como la sumatoria de el MSE entre la matriz de gram de una imagen del estilo y la matriz de gram de la imagen generada, divido un valor en funcion de los cuadrados de la cantidad de canales y tamaño de la imagen.\n",
    "\n",
    "*-content_loss:*\n",
    "\n",
    "Mide el MSE del contenido a modificar (imagen output con rudio blanco) y la imagen dada como input sobre la cual se debe tomar el contenido.\n",
    "\n",
    "*-total_variation_loss:*\n",
    "\n",
    "Esta componente de la loss busca evitar cambios bruscos (componentes de alta frecuencia) en la imagen, logrando un suavizado en la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "1-Gt0ahWvN6q"
   },
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "XCqnju5RvQCo"
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "udEp5h31vRnY"
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-65vcinbvTZ0"
   },
   "outputs": [],
   "source": [
    "# Armamos la loss total\n",
    "loss = K.variable(0.0)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss = loss + content_weight * content_loss(base_image_features,\n",
    "                                            combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :] \n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
    "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "pbz4n1OhvV2K",
    "outputId": "c2b208c6-7ddd-4a40-eeda-525f0809b963"
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JbydbOaVcvU"
   },
   "source": [
    "# 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "*- Explicacion de las celdas:*\n",
    "\n",
    "eval_loss_and_grads es una funcion que obtiene tanto la loss como el gradiente que se definieron en las celdas anteriores.\n",
    "esta funcion es utilizada dentro del evaluator para obtener ambos valores (loss y gradient) y guardarlos en el mismo.\n",
    "Aunque estrictamente solo se necesite la loss en la funcion de loss, siendo que fmin_l_bfgs_b necesita dos funciones diferentes para loss y grad, se calculan ambos (loss y grad) en la funcion de loss y luego se guardan en el evaluator, para posteriormente poder devolver el gradiente en la funcion que corresponda.\n",
    "Calcular estos valores en dos funciones separadas seria ineficiente.\n",
    "\n",
    "la ultima celda instancia el evaluator, carga y preprocesa la imagen base que tiene el contenido y luego por cada una de las iteraciones esperadas (en nuestor caso 100) utiliza la funcion fmin_l_bfgs_b para poder ir minimizando la loss de la y aplicando el estilo.\n",
    "\n",
    "*- Funcion fmin_l_bfgs_b:*\n",
    "\n",
    "Es una funcion que busca minimizar una funcion pasada por parametro usando el algoritmo L-BFGS-B.\n",
    "en el uso que le damos, se le pasa la funcion de loss a minimizar, la imagen preprosesada y hecho un flatten para ser un array y la fprima que indica cual es el gradiente a obtener para minimizar la funcion.\n",
    "\n",
    "*- Diferencia con el paper:*\n",
    "\n",
    "- Se agrega total_variation_loss.\n",
    "- El ratio del pesaje de style vs content es diferente.\n",
    "- Solo se toma 1 capa convolucional para medir el MSE del content y no se intenta ver con cada una de las capaz, en nuestro caso solo tomamos la segunda capa convolucional del quinto bloque, en el paper hacen la prueba con la capa 1 de los 5 bloques. \n",
    "- VGG19 usa max pooling, en el paper se utilizó average pooling\n",
    "- En el paper se utiliza descenso por gradiente para la imagen a generar, aqui se usa fmin_l_bfgs_b\n",
    "\n",
    "*- Alternativas:*\n",
    "\n",
    "Como alternativas podria cambiarse la loss que se utilzia para el contenido, tambien podria usarse alguna otra arquitectura que no sea VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zVE1_qemvZeN"
   },
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Qbl9roIgvdb1"
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb0yOEl-WOE6"
   },
   "source": [
    "# 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n31YBwCVvhAI",
    "outputId": "4c1bf03c-9d66-48ea-93f2-4489fc20beaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 434102170000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_0.png\n",
      "Iteration 0 completed in 387s\n",
      "Start of iteration 1\n",
      "Current loss value: 154287010000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_1.png\n",
      "Iteration 1 completed in 402s\n",
      "Start of iteration 2\n",
      "Current loss value: 87032980000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_2.png\n",
      "Iteration 2 completed in 369s\n",
      "Start of iteration 3\n",
      "Current loss value: 60093575000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_3.png\n",
      "Iteration 3 completed in 327s\n",
      "Start of iteration 4\n",
      "Current loss value: 45415404000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_4.png\n",
      "Iteration 4 completed in 318s\n",
      "Start of iteration 5\n",
      "Current loss value: 37144445000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_5.png\n",
      "Iteration 5 completed in 1336s\n",
      "Start of iteration 6\n",
      "Current loss value: 31288824000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_6.png\n",
      "Iteration 6 completed in 305s\n",
      "Start of iteration 7\n",
      "Current loss value: 27218987000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_7.png\n",
      "Iteration 7 completed in 338s\n",
      "Start of iteration 8\n",
      "Current loss value: 23701008000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_8.png\n",
      "Iteration 8 completed in 322s\n",
      "Start of iteration 9\n",
      "Current loss value: 21294182000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_9.png\n",
      "Iteration 9 completed in 316s\n",
      "Start of iteration 10\n",
      "Current loss value: 19798608000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_10.png\n",
      "Iteration 10 completed in 7776s\n",
      "Start of iteration 11\n",
      "Current loss value: 18306843000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_11.png\n",
      "Iteration 11 completed in 14164s\n",
      "Start of iteration 12\n",
      "Current loss value: 17300322000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_12.png\n",
      "Iteration 12 completed in 10210s\n",
      "Start of iteration 13\n",
      "Current loss value: 16304622000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_13.png\n",
      "Iteration 13 completed in 12234s\n",
      "Start of iteration 14\n",
      "Current loss value: 15547760000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_14.png\n",
      "Iteration 14 completed in 294s\n",
      "Start of iteration 15\n",
      "Current loss value: 14982495000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_15.png\n",
      "Iteration 15 completed in 866s\n",
      "Start of iteration 16\n",
      "Current loss value: 14282224000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_16.png\n",
      "Iteration 16 completed in 4843s\n",
      "Start of iteration 17\n",
      "Current loss value: 13784654000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_17.png\n",
      "Iteration 17 completed in 665s\n",
      "Start of iteration 18\n",
      "Current loss value: 13271118000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_18.png\n",
      "Iteration 18 completed in 297s\n",
      "Start of iteration 19\n",
      "Current loss value: 12839946000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_19.png\n",
      "Iteration 19 completed in 296s\n",
      "Start of iteration 20\n",
      "Current loss value: 12503469000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_20.png\n",
      "Iteration 20 completed in 1324s\n",
      "Start of iteration 21\n",
      "Current loss value: 12164135000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_21.png\n",
      "Iteration 21 completed in 292s\n",
      "Start of iteration 22\n",
      "Current loss value: 11862666000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_22.png\n",
      "Iteration 22 completed in 1291s\n",
      "Start of iteration 23\n",
      "Current loss value: 11570317000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_23.png\n",
      "Iteration 23 completed in 290s\n",
      "Start of iteration 24\n",
      "Current loss value: 11326728000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_24.png\n",
      "Iteration 24 completed in 293s\n",
      "Start of iteration 25\n",
      "Current loss value: 11089185000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_25.png\n",
      "Iteration 25 completed in 878s\n",
      "Start of iteration 26\n",
      "Current loss value: 10857609000.0\n",
      "Image saved as prueba_propia/output/output_at_iteration_26.png\n",
      "Iteration 26 completed in 294s\n",
      "Start of iteration 27\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = result_prefix / ('output_at_iteration_%d.png' % i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkiJtofbWWy1"
   },
   "source": [
    "# 8) Generar imágenes para distintas combinaciones de pesos de las losses. Explicar las diferencias. (Adjuntar las imágenes generadas como archivos separados.)\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "*- Test1:*\n",
    "\n",
    "total_variation_weight = 0.1\n",
    "\n",
    "style_weight = 50\n",
    "\n",
    "content_weight = 1\n",
    "\n",
    "imagen:\n",
    "<img src=\"./content_50_1_0_1/output/output_at_iteration_99.png\">\n",
    "\n",
    "conclusion:\n",
    "\n",
    "En esta prueba se le da mayor peso al estilo que al contenido y es notable como desde las primeras iteraciones, ya se logra una presencia mas importante del estilo en las imagenes generadas comparandolo con las imagenes que propone la notebook.\n",
    "\n",
    "*- Test2:*\n",
    "\n",
    "total_variation_weight = 100\n",
    "\n",
    "style_weight = 10\n",
    "\n",
    "content_weight = 1\n",
    "\n",
    "imagen:\n",
    "<img src=\"./content_10_1_100/output/output_at_iteration_99.png\">\n",
    "\n",
    "conclusion:\n",
    "\n",
    "Es notorio como en esta iteracion se suavizan los los cambios entre pixeles y los colores quedan mucho mas vibrantes, la imagen empieza a parecer blureada y con trazos mucho mas anchos que los de la imagen del estilo\n",
    "\n",
    "*- Test3:*\n",
    "\n",
    "total_variation_weight = 0.1\n",
    "\n",
    "style_weight = 5\n",
    "\n",
    "content_weight = 100\n",
    "\n",
    "imagen:\n",
    "<img src=\"./content_5_100_0_1/output/output_at_iteration_99.png\">\n",
    "\n",
    "conclusion:\n",
    "\n",
    "Al subir el peso del contenido, se nota como el estilo se hace mucho menos presente.\n",
    "\n",
    "*- Test3:*\n",
    "\n",
    "total_variation_weight = 1\n",
    "\n",
    "style_weight = 100\n",
    "\n",
    "content_weight = 4\n",
    "\n",
    "imagen:\n",
    "<img src=\"./content_4_100_1/output/output_at_iteration_99.png\">\n",
    "\n",
    "conclusion:\n",
    "\n",
    "Es notable como al subir todos los valores multiplicadores de loss, no suben todos de la misma forma, sino lo que se ve es que lo importante es el ratio que estos generan entre si.\n",
    "en este test, se logró un estilo similar al de la prueba, con trazos mas anchos por el totla variation weight mayor.\n",
    "\n",
    "\n",
    "# 9) Cambiar las imágenes de contenido y estilo por unas elegidas por usted. Adjuntar el resultado.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "imagen de contenido:\n",
    "\n",
    "<img src=\"./prueba_propia/contenido.jpg\">\n",
    "\n",
    "imagen de estilo:\n",
    "\n",
    "<img src=\"./prueba_propia/estilo.jpg\">\n",
    "\n",
    "creacion final:\n",
    "\n",
    "<img src=\"./prueba_propia/output/output_at_iteration_99.png\">\n",
    "\n",
    "total_variation_weight = 0.1\n",
    "\n",
    "style_weight = 50\n",
    "\n",
    "content_weight = 1\n",
    "\n",
    "conclusion:\n",
    "\n",
    "Se nota claramente como se toma el estilo y el color de la imagen que contiene el estilo, logrando un gran cambio y similitud a esta, manteniendo el contenido de la imagen correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Trabajo Final CNN - Style Transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
